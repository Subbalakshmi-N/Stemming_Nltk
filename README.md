
# Text Pre-Processing with NLTK

This project demonstrates essential **Natural Language Processing (NLP)** techniques using the **Natural Language Toolkit (NLTK)** in Python. The key topics covered include:

- **Tokenization** (Splitting text into sentences and words)
- **Stemming** (Reducing words to their root form)
- **Lemmatization** (Reducing words to their dictionary base form)
- **Stopword Removal** (Filtering out common words)
- **POS Tagging** (Identifying parts of speech in text)
- **Named Entity Recognition (NER)** (Extracting named entities from text)

## üìå Features
- Convert a **paragraph into sentences**.
- Convert **sentences into words**.
- Apply different **tokenizers**.
- Perform **stemming** using Porter, Regexp, and Snowball stemmers.
- Apply **lemmatization** for text normalization.
- Remove **stopwords** to clean text data.
- Identify **parts of speech (POS)** in sentences.
- Perform **Named Entity Recognition (NER)** to extract important entities from text.

## üìú Code Explanation

### 1Ô∏è‚É£ Tokenization
```python
from nltk.tokenize import sent_tokenize, word_tokenize, TreebankWordTokenizer

corpus = """Hello, Welcome to my Github account.
Please do follow my account! you'll become expert in NLP.
"""
print(corpus)

documents = sent_tokenize(corpus)
for sentence in documents:
    print(sentence)

words = word_tokenize(corpus)
for word in words:
    print(word)

tokenizer = TreebankWordTokenizer()
output = tokenizer.tokenize(corpus)
for word in output:
    print(word)
```
---

### 2Ô∏è‚É£ Stemming
#### **Porter Stemmer**
```python
from nltk.stem import PorterStemmer

words = ["eating", "writes", "programming", "history", "finalized"]
stemming = PorterStemmer()
for word in words:
    print(word + " ---> " + stemming.stem(word))
```

#### **Regexp Stemmer**
```python
from nltk.stem import RegexpStemmer

reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)
for word in words:
    print(word + " ---> " + reg_stemmer.stem(word))
```

#### **Snowball Stemmer**
```python
from nltk.stem import SnowballStemmer

snowball = SnowballStemmer('english')
for word in words:
    print(word + " ---> " + snowball.stem(word))
```

---

### 3Ô∏è‚É£ Lemmatization
```python
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
for word in words:
    print(word + " ---> " + lemmatizer.lemmatize(word, pos='v'))
```
---

### 4Ô∏è‚É£ Stopword Removal & Text Cleaning
```python
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

paragraph = """Dr. APJ Abdul Kalam often emphasized the importance of individual contributions..."""
stop_words = set(stopwords.words('english'))
words = word_tokenize(paragraph)
filtered_words = [word for word in words if word.lower() not in stop_words]
print(filtered_words)
```
---

### 5Ô∏è‚É£ POS Tagging
```python
from nltk import pos_tag

words = word_tokenize(paragraph)
pos_tags = pos_tag(words)
print(pos_tags)
```
---

### 6Ô∏è‚É£ Named Entity Recognition (NER)
```python
import nltk
nltk.download('maxent_ne_chunker')
nltk.download('words')

sentence = """The Eiffel Tower was built from 1887 by French engineer Gustave Eiffel, whose company
specialized in building metal frameworks and structures."""

words = nltk.word_tokenize(sentence)
pos_tag = nltk.pos_tag(words)
ne = nltk.ne_chunk(pos_tag)
ne.draw()
```
**Named Entity Recognition (NER)** helps identify important entities like persons, organizations, and locations from text.

---

## üõ† Requirements
Ensure you have **NLTK** installed before running the script:
```bash
pip install nltk
```

To download necessary NLTK datasets, run:
```python
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
```

---

## üöÄ Run the Script
Simply execute the script in a Python environment:

---

## üì¢ Follow Me!
If you found this project helpful, **star** üåü the repository and follow me on GitHub for more NLP projects!

Happy Coding! üéØ


